{
 "metadata": {
  "name": "",
  "signature": "sha256:10f887164b352ea146f4534203813e738d70f2e9ea0fd5aa648275fd9aa582ed"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal is to model the Fatty Acid elongation process using Stochastic Petri Nets. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Syntax and Semantics of Petri Nets"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Simple Petri Nets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A Petri Net is a bi-partite graph consisting of nodes from two distinct classes: Places and Transitions. Places are represented by circles and transitions with squares/rectangles. Each place is associated with a positive integer number which is usually called tokens in the PN literature. Each edge between a transition and a place is also associated with a number which is usually callled multiplicity of that edge. All the places that are connected with the incoming edges of a transition are called pre-places of that transition and all the places that are connected via outgoing edges with the transition are called post-places of that transition. When a transition 'fires' tokens are removed from the pre-places according to the multiplicities of the edges connecting them to the transition and are placed into the post-places again according the mulitplicities of the connecting edges. A transition is said to be enabled if there are enough tokens in its pre-places to be able to fire. In other words the number of tokens in a pre-place of a transition must be higher than the multiplicity of the edge connecting it to the transition.\n",
      "\n",
      "More formally, a Petri Net is a 4-tuple: $PN=(P, T, f, m_0) $\n",
      "where P is the set of places, T is the set of transitions, f is a function describing the connectivity of the net defined as $f: ((P\\times T) \\cup (P\\times T)) \\rightarrow \\mathbb{N}$, and $m_0$ is the initial numbers associated with each place. Collectivelly the set of numbers associated with all the places in the net, $m$, is called the marking of the system."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To illustrate the concepts described above consider a simple example:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Simple PN example](doc/images/example.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the example here the $P = \\{p_1, p_2, p_3\\}$, $T=\\{t1\\}$. In this illustration the tokens associated with each place are shown by circles inside the place. If the multiplicity of an edge is 1 then it is usually omitted. Here the pre-places of transition $t_1$ are $p_1$ and $p_2$ with a single post-place $p_3$. When transition $t_1$ fires 2 tokens from $p_1$ and 1 token from $p_2$ are 'consumed' and 3 tokens are transferred to the post-place $p_3$.\n",
      "\n",
      "This description of the fire process leads nicely into the operational semantics of a Petri Net. The system evolves by repeating the following: at each step one of the enabled transitions is chosen at random, the chosen transition fires and the corresponding changes in token numbers are applied. Notice that although there is an ordering in the execution of transitions which can be interpreted as the evolution of the system in time, the simple PN formalism does not explicitly model time."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Stochastic Petri Nets"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A Stochastic Petri Net (SPN) is an extension to the Petri Net formalism to explicitly model time. A SPN maintains the syntax and semantics described above with the only difference that now each transition $t$ has an associated wait time, $X_t$, which is a random variable distributed according to some probability distribution $f_{X_t}$. The probability distribution function usually used is a negative exponential with some, potentially marking-dependent, firing rate $\\lambda$. As before transitions become enabled and fire as but the only difference is that the associated wait time has to elapse before that happens.\n",
      "The operation is somewhat changed from that of simple PNs. Now at each time point a waiting time is sampled from the probabilistic function of each enabled transition in the net and the transition with the lowest waiting time fires. \n",
      "\n",
      "Formally  an SPN is 5-tuple defined: $SPN=(P, T, f, m_0, \\nu)$. All the definitions are the same as before, in the case of the simple PNs, with the only difference being the addition of the funtion $\\nu$ defined as $\\nu: T \\rightarrow H$. The function $\\nu$ assigns a hazard function to each transition $t$ which defines the potentially marking dependent firing rate $\\lambda_t(m)$ rate for the probability density function, $f_t$, for that transition."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Biological intepretation of SPNs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even though PNs and their extension have been used extensively in Computer Science to model distributed and concurrent systems they were initially inspired by biological networks. They have an especially natural correspondence to metabolic networks. In metabolic networks metabolites are transformed from one to another through the action of enzymes. Each reaction consumes some metabolites and produces others. Metabolites and other chemical species can be associated with places and their absolute numbers can be thought of as the number of tokens for that place. Reactions are then the transitions with the substrates being the pre-places and the product being the post-places. The stoichiometry of the reactions is also captured very naturally by the multiplicity of the edges connecting places and transitions. The rates are then associated with the kinetic rates of the reactions."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SPN model of Fatty Acid elongation process"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Fatty Acid(FA) elongation process"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fatty acids are carboxylic acids with long-chain hydrocarbon side groups. If the carbon bonds in the between the CH groups in their side chains are all single then the FA is called a saturated fatty acid and if they contain double bonds then they are called unsaturated. See picture below:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Chemical composition of saturated and unsaturated fatty acids.](doc/images/fas.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "They are usually defined by the number of carbons in their side chains, so for example a fatty acid with 16 carbons is denoted C16:0 with the number after the colon denoting the location of double bonds. $C_{16}$ and $C_{18}$ are the most naturally occuring FAs. Fatty acids usually have even number of carbon atoms because they are created by the concatenation of $C_2$ units. It is this concatenation process of $C_2$ units to elongate FAs that we are interested in here."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are 3 pathway involved in this elongation process: FA biosynthesis, FA elongation in mitochondria, and FA elongation in Endoplasmic Reticulum(ER). Here we focus on the last two for even-chain unsaturated FAs.\n",
      "The FA elongation pathway in mitochondria only goes up to $C_{16}$ whereas the one in the ER goes on to create very long chain fatty acids $C_n$ for n>16. It is this two elongation pathways, in the mitochondria and ER, that we focus on here."
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "The SPN model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The model is a simplified abstract view of the elongation process as whole so it combines the two elongation pathways described above and describes the elongation from $C_6$ up to $C_{22}$. The model was manually converted from the KEGG pathway model:|"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![SPN model for the FA elongation process](doc/images/fa_elongation_full.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When an Acyl_CoA(n) object is created it has the option of either being converted to a regular $C_n$ fatty acid or go on and be used as a building block for $C_{n+2}$. Since this is a valid SPN it can be executed according to the operational semantics described above. The output of the model are the number of tokens accumulated at the places corresponding to the FAs(C12:0, C14:0 and so on). This model was done with SNOOPY and although SNOOPY is a good tool for a first exploration of the system it doesn't scale very well especially if you want to do multiple simulations to derive some distribution of the output even-chain FAs. For that reason the model was also converted to the PySCeS model definition language so it could easily be read into Python for simulation. The model can be read as follows into a python object which contains the stoichiometry matrix $S$, the pre-places $pre$, the transitions, places."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import fa\n",
      "spn = fa.load_model(\"model/fa_elongation_full.psc\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using model directory: model\n",
        "model/fa_elongation_full.psc loading ..... \n",
        "Parsing file: model/fa_elongation_full.psc\n",
        "Info: No reagents have been fixed\n",
        " \n",
        "Calculating L matrix . . "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ". . . . .  done.\n",
        "Calculating K matrix . . . . . . . no flux conservation\n",
        " done.\n",
        " \n",
        "Integration successful.\n",
        "LSODA time for 20 points: 0.00773406028748\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spn.S"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "array([[-1, -1, -1, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0, -1,  0],\n",
        "       [-1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "       [ 1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0,  1, -1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  1, -1, -1,  0,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0,  0,  1, -1, -1,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1, -1, -1,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, -1, -1, -1],\n",
        "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0],\n",
        "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1]])"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spn.places"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'spn' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-3dc1794af946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'spn' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Module `fa.py` contains two function to simulate the model. The first one is `fa.simulate_gill` which takes the spn object and the number of steps and executes the standard Gillespie simulation. The other second one is `fa.simulate_pn` which proceeds exactly according to the semantics of  SPN execution defined above and returns the output of the execution namely the number of tokens at all the places in the net after the number of steps, defined as argument, is passed or the system has reached a dead state(no enabled transition. Here the SPN is executed for 500 steps."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out = fa.simulate_pn(spn, 500)\n",
      "out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "array([ 0, 51,  0,  0,  1,  0,  0,  0,  1,  0, 39,  0,  7,  0,  0,  1,  0])"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get the model distribution of output FAs over 100 runs with each simulation doing 500 steps:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out_dists = fa.get_model_dists(spn, 100, 500)\n",
      "out_dists"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "array([[  0.,  51.,   0., ...,   1.,   0.,   0.],\n",
        "       [  0.,  50.,   0., ...,   1.,   0.,   1.],\n",
        "       [  0.,  51.,   0., ...,   1.,   0.,   0.],\n",
        "       ..., \n",
        "       [  0.,  51.,   0., ...,   0.,   0.,   0.],\n",
        "       [  0.,  51.,   0., ...,   1.,   0.,   0.],\n",
        "       [  0.,  50.,   0., ...,   1.,   0.,   0.]])"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model inference"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Goal"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The output of the stochastic process defined by the  SPN shown in the previous section is of course dependent on the firing rates of the transitions in the net. In the presence of real measured data for these output metabolites one question that one might ask is can we infer the real firing rates of the transitions in this stochastic process from the data? This is a classic ML inverse problem formulation:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Inference problem formulation](doc/images/inference.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal here is to use the data as evidence to explore the space of all possible SPNs to find the SPN that is closer to the real model that created the data. This is usually involves some comparison between the data simulated by an  SPN and the real data to assess the model's fitness to the data. This is a classical optimisation problem and can be solved to get a point estimate of the parameters of the model with any standard optimisation methods. Alternatively, in order to get a distribution, Bayesian methods can be employed."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data are output relative intensities from MS runs while the output of the SPN are absolute numbers of the output FAs so the comparison is not straight-forward."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Approach 1: Approximate Likelihood"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Approach 2: FA elongation SPN as a sequence of Bernoulli trials"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Point estimates"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We think of the evolution of the system as one token travelling along the net at each step $n$ making a binary decision of whether to be stored as the $C_n$ metabolite or continue and be used as a building block for longer metabolite products $C_{k>n}$:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Binary decision at step $i$](doc/images/coin_toss.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the above case with the token being in one of the intermediates of the pathway (depicted here as a dot in the place). Since it's the only token travelling through the network there are only two transitions enabled in the network at that particular point, $t_1$ and $t_2$, with rates $\\lambda_{t_1}$ and $\\lambda_{t_2}$ respectively. Then, according to the SPN execution semantics, wait times are sampled from negative exponentials with rates $\\lambda_{t_1}$ and $\\lambda_{t_2}$ and the transition with the smallest wait time gets to fire. Then the probability of transition one firing is simply $P(t_1) = \\frac{\\lambda_{t_1} }{\\lambda_{t_1} + \\lambda_{t_2}}$ and similarly for $P(t_2) = \\frac{\\lambda_{t_2} }{\\lambda_{t_1} + \\lambda_{t_2}}$. Since we know that one of the transition will occur then the probabilities of the two transitions are mutually exclusive and we can write $P(t_2) = 1- P(t_1)$. This formulation of the binary decision is essentially a Bernoulli trial with probability of success $p_{t_1}=\\frac{\\lambda_{t_1} }{\\lambda_{t_1} + \\lambda_{t_2}}$ and failure probability $1-p_{t_1}$. This helps us to think of the entire process as a sequence of independent Bernoulli trials(coin tosses) which is a very natural way to think of the process but it also convenient for further analysis:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![FA elongation as a series of Bernoulli trials](doc/images/bernoulli_trials.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this point onward the probability of success for metabolite $A_i$ will be expressed simply as $p_1$ instead of $p_{t_1}$ since we have gone one level up in abstraction thinking now of a series of coin tosses instead of transition firings. The parameters of the model then become simply the set of success probabilities $\\{p_i\\}$. Of course, success probabilities and rates are not the same thing. Each pair of transition rates for a binary decision $i$, $(\\lambda_{t_1}^i, \\lambda_{t_2}^i)$, is related to the corresponding coin toss probabilities: $\\alpha \\frac{\\lambda_{t_1}^i}{\\lambda_{t_2}^i} = \\frac{p_i}{1-p_i}$ which follows from the binary decision argument presented in the previous paragraph. That means that inferring the set of success probabilities for each output metabolite, $\\{p_i\\}$, will give us the correct transition rates for the pair of transitions making up the binary decision for that metabolite up to a proportion. In our case we're not interested in the timing of the entire process but we're rather more interested in the final output of the process(the amount of tokens in each 'sink' after some time or when the input runs out), no matter how long it takes to get there, so using the success probabilities as rates is fine."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Thinking of the entire process as a sequence of Bernoulli trials makes the inference and analysis much easier. How do we infer the success probabilities from a potential dataset?  Imagine a hypothetical scenario where we have a number of sample outputs each with a number of realisations of the process:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Dataset for FA elongation process](doc/images/dataset.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's call this samples $\\times$ metabolites matrix $\\mathbf{D}$. An element of the matrix $D_{ki}$ is the number of instances of metabolite $i$ in sample $k$ where a sample could be a differente individual. Alternatively, using SPN language, $D_{ki}$ is the number of tokens that were accumulated in sink/metabolite $i$ after $\\sum_i D_{ik}$ executions of the SPN. The question is: can we get estimates of the success probabilities for metabolites $\\{p_i\\}$? The ML estimate for a particular $p_i = \\#succ/\\#trials$. The number of successes for a specific sample $k$ is immediately the $D_{ik}$ since $D_{ik}$ tokens passed the binary decision for metabolite $i$ and decided to stay. The number of trials then is the number of all tokens that passed the binary decision. This is a sequential process so the number of trials is $\\sum_{m>=i} D_{km}$ since this is the number of tokens that made the decision to stay *and* all the tokens that went on to become longer FAs. If we have reasons to believe that the samples come from the same populations then we combine the realisations of the process for all samples and get point estimates of the success probabities given by:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ \\hat{p_i} = \\frac{\\sum_k A_{ki}}{\\sum_k \\sum_{m>=i} A_{km}}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The only problem with this approach is that the data we usually get in experiments do not come as absolute numbers. So when we measure the abundances of the different FAs in an individual what we usually get back are relative abundances of the species because the MS/MS experiments only measure relative intensities. This is okay for our ML success probabilities estimation described above. SPN talks about tokens, which are integer numbers associated with each place, so we'll have to transform the relative intensities to relative absolute numbers with the smallest intensity per sample being 1."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have one example dataset with control/treatment mice from a particular study. For start and to create a null mode we can only consider Control individuals. The data are read and trasnformed as described in the previous paragraph:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import infer\n",
      "real_ds = infer.get_dists(\"data/GCFIDrawdata.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "real_ds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "Dataset(data=array([[  2.44593081e+00,   3.23484643e+01,   4.95329330e+00,\n",
        "          4.43373890e-02,   1.05668320e-02,   5.75922680e-02],\n",
        "       [  1.10657030e+00,   2.85385481e+01,   5.31990506e+00,\n",
        "          4.59841190e-02,   1.53257900e-03,   2.72530000e-03],\n",
        "       [  2.12215931e+00,   2.93190254e+01,   5.24719160e+00,\n",
        "          3.80179790e-02,   9.57772800e-03,   2.87964480e-02],\n",
        "       [  1.74750012e+00,   2.91537467e+01,   5.26607423e+00,\n",
        "          0.00000000e+00,   1.42767510e-02,   2.29143790e-02],\n",
        "       [  1.40432259e+00,   2.62459950e+01,   4.08608580e+00,\n",
        "          3.35404440e-02,   1.31937930e-02,   1.90982930e-02],\n",
        "       [  1.76377530e+00,   2.97662736e+01,   4.77585743e+00,\n",
        "          3.81826460e-02,   1.04955400e-02,   1.32851160e-02],\n",
        "       [  1.41727052e+00,   2.65353388e+01,   2.25359918e+00,\n",
        "          1.92489910e-02,   8.71855600e-03,   1.56046610e-02],\n",
        "       [  1.94687394e+00,   2.66434223e+01,   3.22004846e+00,\n",
        "          2.52422280e-02,   7.17795300e-03,   3.78302770e-02],\n",
        "       [  1.30924058e+00,   2.91702386e+01,   4.26513921e+00,\n",
        "          3.71802260e-02,   1.25534410e-02,   1.15855090e-02],\n",
        "       [  7.75174373e-01,   2.58467875e+01,   4.30697532e+00,\n",
        "          4.19309350e-02,   2.53156190e-02,   9.55323800e-03],\n",
        "       [  1.79383051e+00,   2.86205704e+01,   4.55812218e+00,\n",
        "          3.36454770e-02,   7.34038400e-03,   9.04323400e-03],\n",
        "       [  1.43547277e+00,   2.62888231e+01,   6.56685501e+00,\n",
        "          3.84450890e-02,   1.17592300e-02,   2.92765300e-03],\n",
        "       [  1.57887088e+00,   2.64380556e+01,   5.54855737e+00,\n",
        "          3.39278440e-02,   8.98673300e-03,   7.00392300e-03],\n",
        "       [  1.94866147e+00,   2.76171188e+01,   4.54835412e+00,\n",
        "          2.55922790e-02,   6.77083300e-03,   3.25015990e-02],\n",
        "       [  1.73085197e+00,   2.76091806e+01,   5.77382786e+00,\n",
        "          3.53348990e-02,   1.09563910e-02,   1.48369520e-02],\n",
        "       [  7.28521017e-01,   2.54312446e+01,   4.83171504e+00,\n",
        "          3.47439630e-02,   1.72779870e-02,   4.71674800e-03],\n",
        "       [  2.32492609e+00,   2.94637240e+01,   4.45813393e+00,\n",
        "          3.13556480e-02,   7.72713500e-03,   2.63757790e-02],\n",
        "       [  1.69419136e+00,   2.60460243e+01,   5.88224088e+00,\n",
        "          7.50410620e-02,   2.70092530e-02,   2.33878380e-02],\n",
        "       [  1.32010935e+00,   2.57820980e+01,   8.87696679e+00,\n",
        "          1.00388414e-01,   2.58380750e-02,   4.72262900e-03],\n",
        "       [  1.08409892e+00,   2.40435430e+01,   8.98297611e+00,\n",
        "          1.24332510e-01,   4.03890030e-02,   2.97594600e-03],\n",
        "       [  1.32652907e+00,   2.48747907e+01,   5.98405618e+00,\n",
        "          8.13985450e-02,   3.51836330e-02,   1.61594370e-02],\n",
        "       [  1.69086819e+00,   2.94583011e+01,   5.61083523e+00,\n",
        "          9.45149990e-02,   2.13207920e-02,   1.58610640e-02],\n",
        "       [  1.43075517e+00,   2.61030135e+01,   6.38361836e+00,\n",
        "          8.15849940e-02,   2.64439920e-02,   8.03129400e-03],\n",
        "       [  1.12345199e+00,   2.80728667e+01,   7.83572589e+00,\n",
        "          1.26276131e-01,   2.85292790e-02,   0.00000000e+00],\n",
        "       [  8.69636253e-01,   2.53739670e+01,   8.31975059e+00,\n",
        "          1.16230965e-01,   3.43477540e-02,   1.05060090e-02],\n",
        "       [  1.57050018e+00,   2.39172997e+01,   6.71011269e+00,\n",
        "          8.23536300e-02,   2.62251640e-02,   2.68497310e-02],\n",
        "       [  6.68054057e-01,   2.19834685e+01,   3.79911252e+00,\n",
        "          2.25259310e-02,   1.79386710e-02,   7.26448300e-03],\n",
        "       [  9.71033403e-01,   2.43417592e+01,   5.29520738e+00,\n",
        "          5.43425490e-02,   1.35762500e-02,   0.00000000e+00],\n",
        "       [  8.39725157e-01,   2.28812737e+01,   5.88107713e+00,\n",
        "          3.78136760e-02,   2.01220570e-02,   5.07986300e-03],\n",
        "       [  9.70379800e-01,   2.38428904e+01,   3.66112368e+00,\n",
        "          3.09984820e-02,   1.16599090e-02,   1.04274360e-02]]), col_labels=['C14:0', 'C16:0', 'C18:0', 'C20:0', 'C22:0', 'C12:0'])"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Distributions of parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous section we were able to calculate point estimates for the relative pairs of rates at each decision point in the process. Point estimates are fine but distributions give more information about the parameters, for example the range of 'allowed' parameter values. In the traditional Bayesian formulation of the model(parameter) inference problem in the presence of available data the posterior distribution of the model(or equivalently of its parameters that define it) is given by:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ \n",
      "\\begin{align*}\n",
      "P(\\{p_i\\}| \\mathbf{D}) & = \\frac{P(\\mathbf{D} | (\\{p_i\\}) P(\\{p_i\\})}{P(\\mathbf{D})} \\\\\n",
      "& \\propto P(\\mathbf{D} | (\\{p_i\\}) P(\\{p_i\\})\n",
      "\\end{align*}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since every Bernoulli trial is independent we can do the estimation for each $p_i$ independently and the data $D$ in that case would be $(n_i, k_i)$ where $n_i$ is the number of trials and $k_i$ is the number of successes for the $i$th metabolite. So then the likelihood of $(n_i, k_i)$ is $\\mathcal{L}((n_i, k_i) | p_i) = \\binom{n_i}{k_i}{p_i}^{k_i}(1-p_i)^{n_i-k_i}$ and the posterior for parameter $p_i$ is therefore:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\n",
      "\\begin{align*}\n",
      "P(p_i | (n_i, k_i)) &= P((n_i, k_i) | p_i) P(p_i) \\\\\n",
      "& = \\binom{n_i}{k_i}{p_i}^{k_i}(1-p_i)^{n_i-k_i} * P(p_i) \\\\ \n",
      "& = \\binom{n_i}{k_i}{p_i}^{k_i}(1-p_i)^{n_i-k_i}\n",
      "\\end{align*}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where at the last step we ignored the prior on parameter $P(p_i)$ which we can do if we assume that all possible parameter values for $p_i$ are equally likely. We also slightly abuse notation and used an equals sign where we should have used a proportional to sign but we'll continue to do this throughout. This formulation gives us exact posterior for the set of success probabilities $\\{p_i\\}$.\n",
      "\n",
      "One possible problem with this apporach is that since this is a sequential process the binary decisions at the start of the process (small $i$) will have a higher number of trials than those which occur later in the process. Using the exact parameter posterior derived above this will bias the earlier posteriors to have narrower distribution a fact which might or might not reflect some truth about the system. \n",
      "\n",
      "In fact it would be better to go back to the posterior of the set of parameters $P(\\{p_i\\}| \\mathbf{D})$ considering the entire dataset. If we again disregard the priors then this becomes:\n",
      "$$\n",
      "P(\\{p_i\\}| \\mathbf{D}) = P(\\mathbf{D} | (\\{p_i\\}) P(\\{p_i\\})\n",
      "$$\n",
      "so the only thing left to do is to derive a tractable likelihood for the entire process. It turns out this is not so hard to do. Consider a particular sample from a dataset $\\mathbf{D}$ like the one described earlier. Let's call the sample $D$ which is of the form $D=\\{n_i\\}$ where $n_i$ is the number of tokens accumulated in sink/metabolite $i$ after $\\sum_i n_i$ realisations of the stochastic process. This dataset gives us the 'history' of what happened in the process. For example if $n_3=10$ that means that 10 times the process failed at the first and second binary decision and succeeded at the third. If $n_4=5$ that means that 5 times the process failed at the first, second, and third binary decisions and succceeded at the fourth and so on:\n",
      "\n",
      "* $n_1 \\times 1 p_1$\n",
      "* $n_2 \\times 01 (1-p_1)p_2$\n",
      "* $n_3 \\times 001 (1-p_1)(1-p_2)p_3$\n",
      "* $n_4 \\times 0001 (1-p_1)(1-p_2)(1-p_3)p_4$ and so  on.\n",
      "\n",
      "So the probability of getting a token at $A_4$ is $(1-p_1)(1-p_2)(1-p_3)p_4$ and if $n_4=5$ that means that this happened 5 times so $P(n_i=5) = 5 * (1-p_1)(1-p_2)(1-p_3)p_4$. Doing this for all $i$ gives us the likelihood for the entire dataset:\n",
      "\n",
      "$$\n",
      "\\mathcal{L}(D | \\{p_i\\}) = \\prod_i n_i p_i \\prod_{m < i} (1-p_m)\n",
      "$$\n",
      "We can also use log probabilities and then the products become sums to avoid underflows. We can combine the numbers $n_i$ from all the samples in the big dataset $\\mathbf{D}$ and get the same result.\n",
      "Then, again for the time being ignoring the parameter priors, the parameter set posterior becomes:\n",
      "$$\n",
      "\\begin{align*}\n",
      "P(\\{p_i\\} | D) &= \\mathcal{L}(D | \\{p_i\\}) \\\\\n",
      "& = \\prod_i n_i p_i \\prod_{m < i} (1-p_m)\n",
      "\\end{align*}\n",
      "$$\n",
      "\n",
      "Once we have this likelihood function $\\mathcal{L}$, which is very easy to calculate, we can proceed to sample from the parameter posterior using something like MCMC.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Incorporating the TCA cycle"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Try and see if the firing rates or the firing rate distributions(have to use Bayesian method to get dists instead of ML point estimates) are different between clusters from multinomial BHC clustering or Treatment Vs Control. Do they cluster? How do they change? Look at motifs, 3 node binomial motifs!\n",
      "\n",
      "Find firing rates for each sample and then do unsupervised clustering or find the firing rates for each group and see if the cluster agree with the groups."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}